\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{CJKutf8} % 用来支持中文
\usepackage{caption}
\usepackage{graphics}
\usepackage{array}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{amsmath} % 用来写数学公式
\geometry{left=3.2cm, right=3.2cm, top=2.0cm, bottom=2.0cm}
\usepackage{setspace}
\setstretch{1.2} 
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\setcounter{secnumdepth}{0} % section不显示编号
\usepackage{color}
\definecolor{shadecolor}{rgb}{0.94, 0.94, 1.0}
\usepackage{framed}

\title{\textbf{Statistical Inference Assignment 7}}
\author{Junhao Yuan (20307130129)}
\date{\today}

\begin{document}
\begin{CJK}{UTF8}{gbsn}

    \maketitle
    \def \RR{{\mathbb R}}
    \def \EE{{\mathbb E}}
    \def \VV{{\mathbb V}}
    \def \II{{\mathbb I}}
    \def \NN{{\mathcal N}}


    % Problem 1
    \begin{shaded}
        \noindent\textsc{Problem 1.}\par
        Suppose $X_1, \cdots, X_n \mathop{\sim}\limits^{iid} \Gamma(\alpha, \lambda)$, where $\alpha$ is known
        and $\lambda > 0$. Prove that $\bar{X}/\alpha$ is the best unbiased estimator of $1/\lambda$.
    \end{shaded}
    \noindent\textsc{Solution.}\par
    The joint pdf of the sample is
    \begin{align}
        f(X| \alpha, \lambda) & = \prod_{i=1}^n \frac{\lambda^{\alpha}x_i^{\alpha - 1}e^{-\lambda x_i}}{\Gamma(\alpha)} \notag                            \\
                              & = \frac{\lambda^{n\alpha}}{\Gamma(\alpha)^n }\left (\prod_{i=1}^n x_i\right )^{\alpha - 1}e^{-\lambda (\sum x_i)}, \notag
    \end{align}
    which means the joint pdf belongs to the exponential family, and has an open set in the natural parameter space.
    Hence, $T=\sum x_i$ is complete sufficient statistics.
    Since $\bar{X}/\alpha = g(T)$ and $\EE(g(T)) = 1/\lambda$, by the Lehmann-Scheff Theorem, we can conclude
    that $\bar{X}/\alpha$ is the best unbiased estimator of $1/\lambda$.





    % Problem 2
    \begin{shaded}
        \noindent\textsc{Problem 2.}\par
        Suppose $X_1, \cdots, X_n \mathop{\sim}\limits^{iid}\NN (0, \sigma^2)$.
        \begin{itemize}
            \item [(a)] Find the UMVUE of $\sigma$.
            \item [(b)] Find the UMVUE of $\sigma^4$.
        \end{itemize}
    \end{shaded}
    \noindent\textsc{Solution.}\par
    The joint pdf of the sample is
    \begin{align}
        f(X|\sigma^2) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} e^{- x_i^2/(2\sigma^2)} \notag \\
                      & = (2\pi \sigma^2)^{-\frac{n}{2}} e^{-(\sum x_i^2)/(2\sigma^2)} \notag         \\
                      & = (-\mu \pi)^{-\frac{n}{2}} e^{\mu t}, \notag
    \end{align}
    where $t=\sum x_i^2$ and $\mu = -1/(2\sigma^2)\in (-\infty, 0)$. Since the sample distribution belongs to the
    exponential family and there exists an open set in the natural parameter space, $T=\sum X_i^2$ is the complete
    sufficient statistics, and $T/\sigma^2\sim \chi^2_{n}$.
    Therefore, we can come up with the UMVUE of $\sigma$ and $\sigma^4$ based on $T$.
    \begin{itemize}
        \item [(a)] Let $T_1 = T/\sigma^2$, then we have
              \begin{align}
                  \EE(\sqrt{T_1}) & = \int_0^{\infty} \sqrt{t_1} \frac{t_1^{\frac{n}{2} - 1}e^{-t_1/2}}{\Gamma \left (\frac{n}{2}\right )2^{\frac{n}{2}}} d \,t_1 \notag \\
                                  & = \frac{\sqrt{2}\Gamma\left (\frac{n+1}{2} \right )}{\Gamma\left (\frac{n}{2} \right )}.\notag
              \end{align}
              Hence,
              \begin{align}
                  \EE \left (\frac{\Gamma\left (\frac{n}{2} \right ) \sqrt{T} }{\sqrt{2} \Gamma\left (\frac{n+1}{2} \right )} \right ) = \EE(g_1(T)) =  \sigma.\notag
              \end{align}
              Since $g_1(T)$ is an estimator based only on $T$, and is an unbiased estimator of $\sigma$,
              we know that $g_1(T)$ is the UMVUE of $\sigma$.
        \item [(b)]
              Since
              \begin{align}
                  \EE (T_1^2) & = \int_0^{\infty} t_1^2 \frac{t_1^{\frac{n}{2} - 1}e^{-t_1/2}}{\Gamma \left (\frac{n}{2}\right )2^{\frac{n}{2}}} d \,t_1 \notag \\
                              & = \frac{4\Gamma \left (\frac{n}{2} + 2 \right )}{\Gamma\left (\frac{n}{2} \right )} = n(n+2), \notag
              \end{align}
              we have
              \begin{align}
                  \EE \left (\frac{T^2}{n(n+2)} \right ) = \EE(g_2(T)) = \sigma^4.\notag
              \end{align}
              Again, $g_2(T)$ is an estimator only based on $T$, and is an unbiased estimator of $\sigma^4$, thus, is the
              UMVUE of $\sigma^4$.
    \end{itemize}



    % Problem 3
    \begin{shaded}
        \noindent\textsc{Problem 3.}\par
        Suppose $X_1, \cdots, X_n \mathop{\sim}\limits^{iid} Geom(\theta)$, where the pmf of $Geom(\theta)$ is
        \begin{align}
            P(X=i) = \theta (1-\theta)^{i-1}, \qquad i=1, 2, \ldots, \qquad  0 < \theta < 1.
        \end{align}
        Geometric distribution can be interpreted as number of Bernoulli trials ($B(\theta)$) needed to get
        one success.
        \begin{itemize}
            \item [(a)] Show that $T=\sum_{i=1}^n X_i$ is a sufficient statistics for $\theta$, and has pmf
                  \begin{align}
                      P_{\theta}(T=t) = \binom{t-1}{n-1}\theta^n(1-\theta)^{t-n}, t=n, n+1, n+2, \ldots.
                  \end{align}
            \item [(b)] Compute $\EE_{\theta}(T)$ and use it to find the UMVUE of $\theta^{-1}$. (Hint: $\sum_{i=0}^{\infty}z^{i} = \frac{1}{1-z}$ for $|z| < 1$.)
            \item [(c)] Show that $\psi(X_1) = I(X_1=1)$ is an unbaised estimator of $\theta$, use this fact to find the UMVUE of $\theta$.
        \end{itemize}
    \end{shaded}
    \noindent\textsc{Solution.}\par
    \begin{itemize}
        \item [(a)] The joint pdf of the sample is
              \begin{align}
                  f(X|\theta) & = \prod_{i=1}^n \theta (1-\theta)^{x_i-1}   = \left (\frac{\theta}{1 - \theta} \right )^n (1-\theta)^{T} =  \left ( \frac{1-e^{\mu}}{e^{\mu}}\right )^n e^{\mu \, T}, \notag
              \end{align}
              where $\mu = log(1-\theta)$ and $\mu \in (-\infty, 0)$.
              Hence, the sample distribution belongs to the exponential family, and $T$ is a sufficient.
              Furthermore, since the natural parameter space $(-\infty, 0)$ contains an open set, $T$ is also complete.

              The probability of $T$ equals to $t$ can be interpreted as the number of Bernoulli trials needed to get $n$ success,
              which means the $t_{th}$ trial succeed and there had $n-1$ success in the previous $t-1$ Bernoulli trials.
              Therefore,
              \begin{align}
                  P_{\theta}(T=t) = \binom{t-1}{n-1}\theta^n(1-\theta)^{t-n}, t=n, n+1, n+2, \ldots.\notag
              \end{align}
        \item [(b)]
              \begin{align}
                  \EE(T) & = \sum_{t=n}^{\infty} t \binom{t-1}{n-1} \theta^n(1-\theta)^{t-n} \notag                                                                         \\
                         & = \sum_{t=n}^{\infty} (t-n) \binom{t-1}{n-1} \theta^n(1-\theta)^{t-n} + n \sum_{t=n}^{\infty} \binom{t-1}{n-1} \theta^{n}(1-\theta)^{t-n} \notag \\
                         & = \sum_{t=n+1}^{\infty} \binom{t-1}{n-1} \theta^{n}(1-\theta)^{t-n} + n \notag                                                                   \\
                         & = n + n \frac{1-\theta}{\theta} \sum_{t=n+1}^{\infty} \binom{t-1}{n} \theta^{n+1}(1-\theta)^{t-n-1} \notag                                       \\
                         & = \frac{n}{\theta}.\notag
              \end{align}
              Let $g_1(T) = T/n$, then we have $\EE(g_1(T)) = \theta^{-1}$.
              And since $T$ is complete sufficient statistics, $g_1(T)$ is the UMVUE of $\theta^{-1}$.
        \item [(c)]
              Since
              \begin{align}
                  \EE(\psi(X_1))  = P(X_1=1) = \theta, \notag
              \end{align}
              $\psi$ is an unbiased estimator of $\theta$.
              Let $g_2(T) = \EE(\psi(X_1)|T)$, then we have $g_2(T)$ is an unbiased estimator of $\theta$, too.
              And,
              \begin{align}
                  g_2(t) & = \EE(\psi(X_1)|T=t) \notag                                                                                         \\
                         & = \frac{P(x_1=1, \sum_{i=2}^n X_i = t-1)}{P(\sum_{i=1}^n X_i=t)} \notag                                             \\
                         & = \frac{\theta \cdot \binom{t-2}{n-2}\theta^{n-1}(1-\theta)^{t-n}}{\binom{t-1}{n-1}\theta^n(1-\theta)^{t-n}} \notag \\
                         & = \frac{n-1}{t-1},\notag
              \end{align}
              where $t = n, n+1, \ldots$.
              Since $g_2(T)$ is an unbiased estimator only based on $T$, a complete sufficient statistics, we can conclude that
              $g_2(T)$ is the UMVUE of $\theta$.
    \end{itemize}





    % Problem 4
    \begin{shaded}
        \noindent\textsc{Problem 4.}\par
        Suppose $X_1, \cdots, X_n \mathop{\sim}\limits^{iid} f_a(x)$ where
        \begin{align}
            f_a(x) = e^{-(x-a)}\II_{(a, \infty)}(x), \qquad -\infty < a < \infty.
        \end{align}
        Find the UMVUE of $a$.
    \end{shaded}
    \noindent\textsc{Solution.}\par
    The joint pdf of the sample is
    \begin{align}
        f(X|a) & = \prod_{i=1}^n f_a(x_i) \II_{(a, \infty)}(x) \notag            \\
               & = e^{na} \II_{(a, \infty)}(x_{(1)}) \cdot e^{-\sum x_i}. \notag
    \end{align}
    By the factorization theorem, we can know that $T=X_{(1)}$ is the sufficient statistics.
    Then, let's prove that $T$ is also complete.

    It's easy to know that the pdf of $T$ is
    \begin{align}
        f(T=t) & = n f_a(t) (1-F_a(t))^{n-1} = n e^{-n(x-a)} \II_{(a, \infty)}(t).\notag
    \end{align}
    Suppose $\phi(T)$ is any real value function satisfying $\EE(\phi(T))=0$, then we have
    \begin{align}
        \int_a^{\infty} \phi(t) n e^{-n(t-a)}\, d \, t = 0.\label{1}
    \end{align}
    Take derivative with respect to $a$ on both sides of \eqref{1}, then we have
    \begin{align}
        0 & = n \int_{a}^{\infty} \phi(t) ne^{-(t-a)} - \phi(a) = -\phi(a), \qquad \forall a \in \RR.
    \end{align}
    which means $T$ is also complete.

    Since
    \begin{align}
        \EE(T) & = \int_a^{\infty} nte^{-n(t-a)} dt \notag \\
               & = a + \frac{1}{n},\notag
    \end{align}
    $g(T) = T - \frac{1}{n}$ is an unbiased estimator of $a$, and based only on $T$.
    Therefore, $X_{(1)}-\frac{1}{n}$ is the UMVUE of $a$.


\end{CJK}
\end{document}